Title: Dealing with spiky data
date:  2013-05-20 22:58
comments: true
slug: spikes

The first steps to clean a data-set is to remove outliers (or spikes).  Some
spikes are easy to spot with a simple histogram of the data.  Let's take a look
at a velocity time-series with some bad data.

{% notebook spikey_data.ipynb cells[4:5] %}

Can you see the bins that separate themselves from the rest?  Those are
probably spikes.  It is common practice to use a criteria of labeling the data
$3\times \sigma$ larger than the mean as outliers.  However, some spikes might
be "hidden" within the data limits, or one can remove good data together with
bad data if the data is highly variable (or episodic systems).  For a better
discussion on this topic check out Chapter 2 from Emery and Thomson -- Data
Analysis Methods in Physical Oceanography.

One solution is a "two(or more)-pass" criteria.  Also, one might wish to
evaluate small chunks of the data separately.  Here is an example:

{% notebook spikey_data.ipynb cells[2:3] %}

The first compute the statistics of the data ($\mu$ and $\sigma$) and marks
(but do not exclude yet) data that deviates more than $n1 \times \sigma$ from
the mean.  The second pass recompute the data statistics but now removing the
data that deviates more than $n2 \times \sigma$ from the mean.

The code above makes use of [NumPy stride tricks](http://stackoverflow.com/questions/4936620/using-strides-for-an-efficient-moving-average-filter "NumPy stride tricks")
to avoid loops while computing $\mu$ and $\sigma$ for each **block**.

{% notebook spikey_data.ipynb cells[5:6] %}

The "two-pass" method did well for *u* while the $3\times \sigma$ criteria
failed not only to identify all the spikes, but also removed good data from the
time series.  On the other had, the $3\times \sigma$ criteria removed two bad
values from *v* while the two pass could one find one.  Still both methods left
behind two small spikes on *v*.

There are probably better ways of doing this.  Also, there are methods that are
more statistically robust and more efficient at catching bad values.  Still,
the message of this post is not actually an algorithm to remove bad data, but:

"To identify errors, it is necessary to examine all of the data in visual form
and to get a "feel" for the data."  -- directly from Emery and Thomson book.

In other words, there is no silver bullet!

http://www.johndcook.com/blog/2013/05/30/there-are-no-outliers/
